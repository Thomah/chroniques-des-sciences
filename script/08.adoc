== Acte 8

=== Scène 1

[.text-center]
**Prof. Calvet**

_Slide 8.0_

Grâce au travail de Quevedo, les machines sont désormais capables d’effectuer des calculs complexes mais il continuait à rêvaer d’un monde où des machines, non seulement exécutent, mais apprennent, anticipent, ou même raisonnent.

Et à mesure que les nouveaux composants électroniques – diodes, triodes, puis transistors – remplaçaient les engrenages et les contacts physiques, les rêves d'hier sont devenus les bases d'une révolution.

Et cette révolution a trouvé son origine au XXe siècle, dans des laboratoires d’universités et des hangars militaires, là où l’informatique moderne a commencé.

_Slide 8.1_

Voici Alan Turing, un esprit brillant du XXe siècle. Matheux, logicien, mais surtout visionnaire.

_Slide 8.2_

Il a conçu un modèle théorique capable de résoudre n’importe quel problème calculable, une simple machine idéale, composée de bandes de papier et de têtes de lecture-écriture. Une idée révolutionnaire : avec des instructions simples et une mémoire, cette machine abstraite pouvait imiter n’importe quelle autre machine…

[.text-center]
**Jack**

Attendez… donc, si je comprends bien, la machine de Turing, c’est… genre, l’ancêtre de nos ordinateurs ?

[.text-center]
**Prof. Calvet**

Exactement ! Elle ne "fonctionnait" pas au sens matériel, mais elle posait de solides bases théoriques. 

_Slide 8.3_

Et ces bases théoriques ont été rapidement mises en pratiques sur de véritables ordinateurs électroniques. Le premier colosse ? L’ENIAC, construit en 1945. Imaginez : 27 tonnes de circuits et d’ampoules pour faire moins que ce que vous faites aujourd’hui avec un simple smartphone !

_Slide 8.4_

Mais derrière cette merveille technique, un groupe de six femmes remarquables, souvent oubliées de l’histoire : les ENIAC Girls. Programmant avec des câbles et des fiches, elles ont défini le fonctionnement de cette machine tentaculaire sans aucune documentation ni assistance technique.

[.text-center]
**Jack**

Ah, donc les vrais génies de l’ENIAC… c’était elles ?

[.text-center]
**Prof. Calvet**

Exactement ! Elles ont eu un rôle crucial, mais hélas sous-estimé pendant des décennies.

_Slide 8.5_

Mais que signifient toutes ces avancées ? Pour Alan Turing, c’était l’idée qu’un jour, les machines pourraient peut-être… penser. Il a imaginé un test : le jeu de l’imitation. Si une machine peut convaincre un humain qu’elle pense, peut-on dire qu’elle est intelligente ?

[.text-center]
**Jack**

Un test ? Avec des machines qui parlent ? Alors, vous venez de prédire... moi ! En plus beau, bien sûr.

[.text-center]
**Prof. Calvet**

Pour mieux comprendre, nous allons réaliser l'un de ces tests. J'aurais besoin de 2 volontaires.

L'un de vous aura les yeux bandés.

Il y aura 3 questions qui vont s'afficher à l'écran. L'un de nous y répondra honnêtement et l'autre se contentera de lire les réponses de Jack, qui sera muet pour cette expérience.

La personne ayant les yeux bandés devra identifier qui de nous deux parle au nom de Jack.

N’oubliez pas : le but de Turing n’était pas de démontrer que les machines pensent, mais qu’elles imitent si bien les humains que nous devons nous poser une question fascinante : où commence réellement la pensée ?

[.text-center]
**Jack**

Et surtout, imaginez ceci : un jour, nous pourrons confondre humains et machines. Serez-vous à l’aise avec cette idée ?

[.text-center]
**Prof. Calvet**

Maintenant que vous avez expérimenté ce jeu d’imitation, demandez-vous : que signifie "penser" pour vous ? Alan Turing lui-même ne cherchait pas qu’une réponse scientifique… mais une réflexion sur ce que nous sommes !

Mais dans les années 1950, un homme audacieux s'est attaqué à cette idée. Son nom : John McCarthy.

_Slide 8.6_

McCarthy a lancé l’intelligence artificielle. C'est en 1956, qu'il invente ce terme ! Sa vision ? Créer des machines qui n’agissent pas simplement sur des instructions fixes, mais qui apprennent, raisonnent et prennent des décisions.

À l’époque, elles étaient loin d’atteindre ce but, mais c'était le début. McCarthy croyait que des programmes bien conçus pouvaient imiter certains aspects de la réflexion humaine. C’était ambitieux ! Par exemple, des jeux comme les échecs ont servi à prouver que les machines pouvaient analyser des stratégies complexes. Les bases étaient posées : on entrait dans l’ère des machines intelligentes.

=== Scène 2

[.text-center]
**Prof. Calvet**

_Slide 8.7_

Mais attention : il ne suffit pas que les machines pensent. Pour les comprendre et les guider, il fallait aussi résoudre des problèmes liés à la communication des données, et aux montagnes d'informations qu’elles manipulent. Des chercheurs comme Claude Shannon et des ingénieurs comme ceux de l’ENIAC s'y sont attelés. 

Il commence par poser ceci :
“Le problème fondamental de la communication est de reproduire en un point, soit exactement, soit approximativement, un message recueilli en un autre point.”

Imaginez : vous devez transmettre un message à distance. Des obstacles peuvent altérer ce message : bruit, interférences, erreurs… Shannon ne veut pas simplement éviter la confusion, il cherche à codifier l'information pour qu'elle puisse résister à tout cela. Et c'est ainsi qu'il crée...

_Slide 8.8_

La théorie de l’information ! Une méthode brillante qui définit les bases de la communication moderne : des téléphones portables aux satellites, des ordinateurs à l'Internet, tout repose sur ses travaux.

Le principe, c'est que n'importe quelle information – un texte, un son, une image – pouvait être réduite à des 0 et des 1, réutilisant par la même occasion les travaux de George Boole. Ce binaire devint la langue de l’informatique. Et comme toute langue, elle doit avoir des règles.

[.text-center]
**Jack**

Imaginez ceci : au lieu de parler anglais, chinois ou espagnol, les machines se comprennent avec une suite infinie des courants électriques allumés ou éteints. lumières allumées ou éteintes.

https://www.youtube.com/watch?v=_gFwe6HiyFI

_Slide 8.9_

[.text-center]
**Prof. Calvet**

Et en pratique ça donne quoi ? Essayons de convertir un mot en binaire. Prenons quelque chose de court, comme par exemple le mot "vie".

_Slide 8.9.0_

En anglais, ça se dit "life". Eh bien en binaire, ça se dit... Jack tu veux bien ?

_Slide 8.9.1_

[.text-center]
**Jack**

01010110 01001001 01000101

[.text-center]
**Prof. Calvet**

Vous imaginez qu'écrire des 1 et des 0 à la chaîne, les programmeurs de l'époques n'en étaient pas fan. C'est pour ça qu'on a inventé un premier langage : l'assembleur.

_Slide 8.10_

Un programme en assembleur ressemble à ça. Plus facile que des 0 et 1 à lire pour les humains et facilement traduisible pour les machines.

_Slide 8.11_

Ce langage a permis de grandes choses. Un exemple parmis tant d'autres : Margaret Hamilton et son équipe ont écrit pendant les années 1960 les logiciels de pilotage pour la fusée Apollo 11.

C'était une véritable prouesse car à l'époque, il n'y avait pas de formations, de cours, ou même de documentation sur le génie logiciel.

_Slide 8.12_

Et lorsqu'on envoit une fusée dans l'espace, on n'a pas le droit à l'erreur. 

=== Scène 3

[.text-center]
**Jack**

Mais, Professeur, Comment fait-on lorsqu'on a une technologie incroyablement complexe comme l'ENIAC mais qu'elle occupe la taille d'une maison ?

_Slide 8.12_

[.text-center]
**Prof. Calvet**

Eh bien, Jack, je crois qu'il est temps de penser petit ! La miniaturisation est au cœur de ce qui nous attend. Bienvenue dans la révolution des semi-conducteurs, une étape essentielle pour rendre tout ce progrès... portable.

Tout cela commence dans les années 1940, avec un composant électronique à peine visible à l’œil nu : le transistor. Inventé par John Bardeen, Walter Brattain et William Shockley, ce petit composant a changé la donne. A tel point qu'ils ont reçu un prix Nobel de Physique en 1956.

Les transistors se sont vite retrouvés intégrés dans des circuits intégrés ou puces électroniques, rendant les machines de plus en plus petites et rapides.

_Slide 8.12.0_

[.text-center]
**Jack**

Ainsi, on passe de machines de la taille d’un bâtiment... à une puce qu’on peut tenir entre deux doigts. Franchement, pourquoi ça leur a pris si longtemps ?

_Slide 8.13_

[.text-center]
**Prof. Calvet**

La patience, Jack. En 1958, l’Américain Jack Kilby met au point le premier circuit intégré. À partir de là, le développement est exponentiel ! Et c’est grâce à cela que nous avons pu donner naissance à des ordinateurs vraiment personnels, mais tout cela a un coût.

(Se tournant vers l’audience.) Saviez-vous que pour produire ces fameuses puces électroniques, on a besoin de matériaux rares comme le lithium, le tantale, ou encore le cobalt ?

_Slide 8.14_

[.text-center]
**Jack**

Ah, les fameux "terres rares". N’est-ce pas ironique ? On les appelle rares alors qu’elles sont devenues absolument essentielles pour fabriquer à peu près tout : les smartphones, ordinateurs... et, eh bien, moi.

[.text-center]
**Prof. Calvet**

Exactement. Mais leur extraction n’est pas sans conséquences. Prenons le silicium. C’est vrai, il est abondant dans le sable, mais sa purification nécessite de hautes températures et beaucoup d’énergie.

_Slide 8.15_

Et pour alimenter nos besoins toujours croissants en technologies, on creuse. Et ces mines, souvent situées en Afrique ou en Amérique du Sud, détruisent des écosystèmes, polluent les eaux et affectent la vie des communautés locales.

[.text-center]
**Jack**

Voilà, je suis coupable. Vous voulez m’éteindre ?

[.text-center]
**Prof. Calvet**

_Slide 8.16_

Pas question, Jack. Ce serait trop simple. Ce qu’il faut, c’est repenser comment on fabrique, utilise et recycle ces technologies. Car on produit presque 50 millions de tonnes de déchets électroniques par an ! Imaginez une montagne de vieux téléphones et d’ordinateurs. Et seulement une fraction de tout cela est recyclée correctement. Le reste ? Brûlé, envoyé dans des décharges… Des déchets qui contiennent pourtant encore beaucoup de ces fameux matériaux précieux.

[.text-center]
**Jack**

Mais peut-on pour autant blâmer les inventeurs de ces nouvelles technologies ?

[.text-center]
**Prof. Calvet**

Bien sûr que non. Mais l'Histoire et les conséquences qu'on en tire doivent nous servir de leçon. Lorsqu'on innove, qu'on créé, sans même parler d'inventer quelque chose de révolutionnaire, tous les impacts doivent être réfléchis : technologiques bien sûr, mais aussi humains et environnementaux.
